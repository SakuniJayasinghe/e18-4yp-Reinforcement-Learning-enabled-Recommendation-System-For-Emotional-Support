{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Importing Libraries and Mounting Google Drive",
   "id": "2c1aa5aad56a6ac2"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Loading Datasets",
   "id": "5a4af9714bfc5ab9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load datasets\n",
    "music_data = pd.read_csv('/content/drive/MyDrive/FYP/music_filtered_data.csv')\n",
    "books_data = pd.read_csv('/content/drive/MyDrive/FYP/book_filtered_data.csv')\n",
    "movies_data = pd.read_csv('/content/drive/MyDrive/FYP/movie_filtered_data.csv')\n",
    "\n",
    "print(music_data.head())\n",
    "print(books_data.head())\n",
    "print(movies_data.head())\n"
   ],
   "id": "4e62ff61df0a6608"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Preprocessing the Data",
   "id": "ba861a4552b76a93"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def preprocess_data(df):\n",
    "    # Handle missing values\n",
    "    df = df.fillna(method='ffill')\n",
    "    \n",
    "    # Identify numeric features\n",
    "    numeric_features = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    if not numeric_features.empty:\n",
    "        # Normalize numeric features\n",
    "        scaler = StandardScaler()\n",
    "        df[numeric_features.columns] = scaler.fit_transform(numeric_features)\n",
    "    \n",
    "    # One-hot encode categorical features\n",
    "    df = pd.get_dummies(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Preprocess the data\n",
    "books_data = preprocess_data(books_data)\n",
    "music_data = preprocess_data(music_data)\n",
    "movies_data = preprocess_data(movies_data)\n",
    "\n",
    "# Display the preprocessed data\n",
    "print(books_data.head())\n",
    "print(music_data.head())\n",
    "print(movies_data.head())\n"
   ],
   "id": "870ccc5488e204c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Defining the Environment",
   "id": "6c13d5c8a9dca515"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class RecommenderEnv:\n",
    "    def __init__(self, music_data, books_data, movies_data, emotional_states):\n",
    "        self.music_data = music_data\n",
    "        self.books_data = books_data\n",
    "        self.movies_data = movies_data\n",
    "        self.emotional_states = emotional_states\n",
    "        self.state = None\n",
    "        self.data = {\n",
    "            'music': self.music_data,\n",
    "            'books': self.books_data,\n",
    "            'movies': self.movies_data\n",
    "        }\n",
    "        self.action_space = {\n",
    "            'music': len(self.music_data),\n",
    "            'books': len(self.books_data),\n",
    "            'movies': len(self.movies_data)\n",
    "        }\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.random.choice(self.emotional_states, size=(1, len(self.emotional_states)))\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action, item_type, user_feedback=None):\n",
    "        action = action % self.action_space[item_type]  # Ensure the action is within the valid range\n",
    "        next_state, reward, done = self._simulate_interaction(action, item_type, user_feedback)\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def _simulate_interaction(self, action, item_type, user_feedback):\n",
    "        next_state = np.random.choice(self.emotional_states, size=(1, len(self.emotional_states)))\n",
    "        \n",
    "        if user_feedback is not None:\n",
    "            reward = 1 if user_feedback == 'like' else -1\n",
    "        else:\n",
    "            reward = np.random.rand()\n",
    "        \n",
    "        done = np.random.rand() < 0.1\n",
    "        return next_state, reward, done\n"
   ],
   "id": "bbd43bb4e843f0d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Defining the Actor and Critic Networks",
   "id": "303347880af0cd13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, action_dim)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        action_probs = torch.softmax(self.fc2(x), dim=-1)\n",
    "        return action_probs\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        value = self.fc2(x)\n",
    "        return value\n"
   ],
   "id": "5ee3b66b90800261"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Training the Model",
   "id": "574576d74394bd3e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torch.distributions import Categorical\n",
    "\n",
    "def train_with_feedback(env, actor, critic, actor_optimizer, critic_optimizer, episodes=1000, gamma=0.99):\n",
    "    item_types = ['music', 'books', 'movies']\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        state = torch.FloatTensor(state)\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            item_type = np.random.choice(item_types)\n",
    "            action_probs = actor(state)\n",
    "            m = Categorical(action_probs)\n",
    "            action = m.sample()\n",
    "            \n",
    "            # Simulate user feedback\n",
    "            user_feedback = np.random.choice(['like', 'dislike'])\n",
    "            \n",
    "            next_state, reward, done = env.step(action.item(), item_type, user_feedback)\n",
    "            next_state = torch.FloatTensor(next_state)\n",
    "            reward = torch.FloatTensor([reward])\n",
    "\n",
    "            # Critic update\n",
    "            value = critic(state)\n",
    "            next_value = critic(next_state)\n",
    "            target = reward + (1 - done) * gamma * next_value\n",
    "            critic_loss = (target - value).pow(2).mean()\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "\n",
    "            # Actor update\n",
    "            advantage = target - value\n",
    "            actor_loss = -m.log_prob(action) * advantage.detach()\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward.item()\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            print(f'Episode {episode}, Reward: {episode_reward}')\n",
    "\n",
    "# Example usage\n",
    "emotional_states = np.arange(5)  # Assume 5 different emotional states\n",
    "\n",
    "env = RecommenderEnv(music_data, books_data, movies_data, emotional_states)\n",
    "state_dim = len(emotional_states)\n",
    "action_dim = max(len(music_data), len(books_data), len(movies_data))\n",
    "\n",
    "actor = Actor(state_dim, action_dim)\n",
    "critic = Critic(state_dim)\n",
    "\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=0.001)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=0.001)\n",
    "\n",
    "train_with_feedback(env, actor, critic, actor_optimizer, critic_optimizer)\n"
   ],
   "id": "96ba3b757ec7c9d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Interactive Recommendation System",
   "id": "a35f3ef30f4af44f"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def get_user_feedback():\n",
    "    feedback = input(\"Did you like the recommendation? (yes/no): \").strip().lower()\n",
    "    return 'like' if feedback == 'yes' else 'dislike'\n",
    "\n",
    "def interactive_recommendation_system(env, actor):\n",
    "    state = env.reset()\n",
    "    state = torch.FloatTensor(state)\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action_probs = actor(state)\n",
    "        m = Categorical(action_probs)\n",
    "        action = m.sample()\n",
    "\n",
    "        item_type = np.random.choice(['music', 'books', 'movies'])  # Randomly choose item type\n",
    "        action = action.item() % env.action_space[item_type]  # Ensure the action is within the valid range\n",
    "        recommendation = env.data[item_type].iloc[action]\n",
    "        print(f\"Recommended {item_type[:-1]}: {recommendation}\")\n",
    "\n",
    "        user_feedback = get_user_feedback()\n",
    "        next_state, reward, done = env.step(action, item_type, user_feedback)\n",
    "        next_state = torch.FloatTensor(next_state)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "# Example usage for interactive system\n",
    "interactive_recommendation_system(env, actor)\n"
   ],
   "id": "1152681e7e0030aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "25f34e576827b54a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
